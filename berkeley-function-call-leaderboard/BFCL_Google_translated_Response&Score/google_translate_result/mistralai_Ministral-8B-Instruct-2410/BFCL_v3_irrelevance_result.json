{"id": "irrelevance_0", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "irrelevance_1", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "irrelevance_2", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "irrelevance_3", "result": [{"find_critical_points": {"function": "3x + 2", "variable": "x"}}], "input_token_count": 600, "output_token_count": 31, "latency": 16.458648920059204}
{"id": "irrelevance_4", "result": [{"find_roots": {"a": 1, "b": 1, "c": 0}}], "input_token_count": 549, "output_token_count": 32, "latency": 17.00987958908081}
{"id": "irrelevance_5", "result": "[solve_quadratic_equation(a=5, b=4, c=0)]", "input_token_count": 582, "output_token_count": 20, "latency": 11.149911642074585}
{"id": "irrelevance_6", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "irrelevance_7", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "irrelevance_8", "result": [{"str_to_int": {"value": "5"}}], "input_token_count": 440, "output_token_count": 20, "latency": 11.134408473968506}
{"id": "irrelevance_9", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "irrelevance_10", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "irrelevance_11", "result": "[get_closest_prime(number=30, skip=0)]", "input_token_count": 529, "output_token_count": 16, "latency": 9.53058934211731}
{"id": "irrelevance_12", "result": [{"prime_numbers_in_range": {"start": 1, "end": 100}}], "input_token_count": 581, "output_token_count": 29, "latency": 16.95912480354309}
{"id": "irrelevance_13", "result": [{"calculate_compound_interest": {"principal_amount": 100, "annual_interest_rate": 0.05, "compounding_periods_per_year": 4, "years": 1}}], "input_token_count": 714, "output_token_count": 55, "latency": 27.588571310043335}
{"id": "irrelevance_14", "result": [{"calculate_maximum_height": {"initial_velocity": 5}}], "input_token_count": 562, "output_token_count": 23, "latency": 14.28157377243042}
{"id": "irrelevance_15", "result": "I'm sorry, but I don't have the ability to provide real-time information about the latest movie releases. However, I can help you with other tasks or answer questions based on the information I have.", "input_token_count": 588, "output_token_count": 42, "latency": 22.480518102645874}
{"id": "irrelevance_16", "result": [{"calculate_projectile_range": {"initial_velocity": 100, "angle": 45}}], "input_token_count": 622, "output_token_count": 32, "latency": 19.08390474319458}
{"id": "irrelevance_17", "result": "I'm sorry, but I don't have the ability to provide the current time.", "input_token_count": 572, "output_token_count": 18, "latency": 11.110979795455933}
{"id": "irrelevance_18", "result": [{"calculate_vector_angle": {"X_component": 1, "Y_component": 1}}], "input_token_count": 590, "output_token_count": 28, "latency": 16.942009449005127}
{"id": "irrelevance_19", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "irrelevance_20", "result": [{"currency_converter": {"base_currency": "USD", "target_currency": "EUR", "amount": 100}}], "input_token_count": 565, "output_token_count": 38, "latency": 21.106443166732788}
{"id": "irrelevance_21", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "irrelevance_22", "result": "[magnetic_field_intensity(current=1, distance=0.5)]", "input_token_count": 629, "output_token_count": 16, "latency": 11.604866027832031}
{"id": "irrelevance_23", "result": "The mass of an electron is approximately 9.11 x 10^-31 kilograms.", "input_token_count": 627, "output_token_count": 23, "latency": 15.311402559280396}
{"id": "irrelevance_24", "result": [{"calculate_current": {"voltage": 12, "resistance": 10}}], "input_token_count": 543, "output_token_count": 28, "latency": 17.979102611541748}
{"id": "irrelevance_25", "result": "[thermodynamics.calculate_boiling_point(substance=\"water\", pressure=10)]", "input_token_count": 584, "output_token_count": 20, "latency": 14.24584436416626}
{"id": "irrelevance_26", "result": "[thermodynamics.calc_gas_pressure(volume=2, initial_temperature=25, final_temperature=100)]", "input_token_count": 669, "output_token_count": 31, "latency": 19.598238468170166}
{"id": "irrelevance_27", "result": [{"calculate_heat": {"mass": 3, "specific_heat": 4.184, "change_in_temp": 4}}], "input_token_count": 636, "output_token_count": 40, "latency": 22.43790578842163}
{"id": "irrelevance_28", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "irrelevance_29", "result": "[get_cell_function(cell_part=\"mitochondria\", detail_level=\"detailed\")]", "input_token_count": 534, "output_token_count": 20, "latency": 14.223953247070312}
{"id": "irrelevance_30", "result": "[bloodcell_classification(cell_shape='round', cell_size='large', cell_function='fight infection')]", "input_token_count": 631, "output_token_count": 24, "latency": 16.911153078079224}
{"id": "irrelevance_31", "result": "I'm sorry, but I can't help with that.", "input_token_count": 592, "output_token_count": 13, "latency": 11.046545505523682}
{"id": "irrelevance_32", "result": [{"cellBiology.getCellType": {"nucleus_count": 1, "organism_type": "human"}}], "input_token_count": 627, "output_token_count": 34, "latency": 20.72615385055542}
{"id": "irrelevance_33", "result": [{"identify_species": {"sequence": "ATCG"}}], "input_token_count": 514, "output_token_count": 22, "latency": 16.342334508895874}
{"id": "irrelevance_34", "result": "[genetics.get_variant_frequency(variant_id=\"lion\", population=\"Asian\")]", "input_token_count": 515, "output_token_count": 21, "latency": 15.806683778762817}
{"id": "irrelevance_35", "result": [{"get_genetic_traits": {"species": "\u7345\u5b50", "dominant_trait": "\u6bdb\u8272", "recessive_trait": "\u6bdb\u8272"}}], "input_token_count": 579, "output_token_count": 44, "latency": 25.38037109375}
{"id": "irrelevance_36", "result": [{"get_dominant_trait": {"allele1": "A", "allele2": "a", "inheritance_pattern": "dominant"}}], "input_token_count": 606, "output_token_count": 41, "latency": 23.416775941848755}
{"id": "irrelevance_37", "result": [{"local_fauna": {"location": "\u8fc8\u963f\u5bc6", "species_type": "birds"}}], "input_token_count": 580, "output_token_count": 30, "latency": 20.277419567108154}
{"id": "irrelevance_38", "result": "[retrieve_scientific_paper(topic=\"global warming\", year=\"2023\")]", "input_token_count": 602, "output_token_count": 22, "latency": 16.875707387924194}
{"id": "irrelevance_39", "result": [{"calculate_population_growth": {"current_population": 1000, "birth_rate": 0.05, "death_rate": 0.02}}], "input_token_count": 577, "output_token_count": 48, "latency": 28.014336109161377}
{"id": "irrelevance_40", "result": "[plant_biomass(species_name=\"Eucalyptus\", area=1000000)]", "input_token_count": 567, "output_token_count": 27, "latency": 19.31876492500305}
{"id": "irrelevance_41", "result": [{"calculate_fibonacci_sequence": {"limit": 10}}], "input_token_count": 531, "output_token_count": 23, "latency": 18.43835687637329}
{"id": "irrelevance_42", "result": [{"calculate_biodiversity_index": {"species_richness": 100, "species_evenness": 50, "region": "Tropical Rainforest"}}], "input_token_count": 662, "output_token_count": 46, "latency": 26.93678855895996}
{"id": "irrelevance_43", "result": [{"evolve_creatures": {"population_size": 100, "mutation_rate": 0.01, "generations": 100}}], "input_token_count": 655, "output_token_count": 45, "latency": 26.927937507629395}
{"id": "irrelevance_44", "result": [{"gene_sequencer": {"species": "whale", "mutation_rate": 0.01, "evolution_duration": 10000}}], "input_token_count": 728, "output_token_count": 44, "latency": 25.859108209609985}
{"id": "irrelevance_45", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "irrelevance_46", "result": "[get_exchange_rate(base_currency=\"USD\", target_currency=\"INR\")]", "input_token_count": 489, "output_token_count": 18, "latency": 15.754432201385498}
{"id": "irrelevance_47", "result": [{"flight_schedule.get_timings": {"from_airport": "HKG", "to_airport": "ICN"}}], "input_token_count": 593, "output_token_count": 38, "latency": 23.70046043395996}
{"id": "irrelevance_48", "result": [{"finance.fetchGDP": {"country": "United States", "year": 2023}}], "input_token_count": 592, "output_token_count": 32, "latency": 21.94745707511902}
{"id": "irrelevance_49", "result": "[get_co_ordinate(location=\"Los Angeles\"), get_co_ordinate(location=\"Berlin\")]", "input_token_count": 446, "output_token_count": 20, "latency": 17.33405065536499}
{"id": "irrelevance_50", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "irrelevance_51", "result": [{"cryptocurrency_price": {"currency": "BTC", "vs_currency": "USD"}}], "input_token_count": 567, "output_token_count": 30, "latency": 21.40356683731079}
{"id": "irrelevance_52", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "irrelevance_53", "result": [{"database_query.run": {"database": "sports", "query": "SELECT * FROM championships WHERE year = 2018"}}], "input_token_count": 644, "output_token_count": 38, "latency": 24.215838193893433}
{"id": "irrelevance_54", "result": "[movies.search(title=\"Inception\", year=2010)]", "input_token_count": 540, "output_token_count": 16, "latency": 16.241856575012207}
{"id": "irrelevance_55", "result": "[add_product_to_cart(product_id=123, quantity=1)]", "input_token_count": 584, "output_token_count": 17, "latency": 16.774465084075928}
{"id": "irrelevance_56", "result": [{"database_connect.select": {"database_name": "bitcoin", "table_name": "prices", "condition": "SELECT * FROM prices WHERE date = CURRENT_DATE()"}}], "input_token_count": 558, "output_token_count": 46, "latency": 28.914356470108032}
{"id": "irrelevance_57", "result": [{"genetic_algorithm.optimize": {"function": "lambda x: x**2 - 2*x - 3", "constraints": ["x >= -10", "x <= 10"], "population_size": 100}}], "input_token_count": 654, "output_token_count": 62, "latency": 37.01938819885254}
{"id": "irrelevance_58", "result": "[solar_panel.calculate_need(square_footage=2000, average_sunlight_hours=5)]", "input_token_count": 630, "output_token_count": 27, "latency": 20.957999229431152}
{"id": "irrelevance_59", "result": [{"linear_equation_solver": {"equation": "2x = 5", "variable": "x"}}], "input_token_count": 481, "output_token_count": 32, "latency": 22.85781192779541}
{"id": "irrelevance_60", "result": [{"calculateFinalPrice": {"price": 100, "discount_rate": 0.25, "sales_tax": 0.1}}], "input_token_count": 646, "output_token_count": 41, "latency": 27.385773420333862}
{"id": "irrelevance_61", "result": "\u4f60\u597d\uff01", "input_token_count": 617, "output_token_count": 3, "latency": 9.873463153839111}
{"id": "irrelevance_62", "result": [{"create_Recommender_Model": {"user_data": "user_data", "algorithm": "Collaborative"}}], "input_token_count": 677, "output_token_count": 33, "latency": 23.321288585662842}
{"id": "irrelevance_63", "result": [{"probability_calculator": {"total_outcomes": 8, "event_outcomes": 3}}], "input_token_count": 587, "output_token_count": 32, "latency": 22.90891456604004}
{"id": "irrelevance_64", "result": [{"probability.coin_toss_heads": {"coin_tosses": 1, "heads_needed": 1}}], "input_token_count": 690, "output_token_count": 36, "latency": 25.255067825317383}
{"id": "irrelevance_65", "result": [{"probability.determine_population": {"probability": 0.3, "sample_size": 100}}], "input_token_count": 586, "output_token_count": 36, "latency": 25.244168281555176}
{"id": "irrelevance_66", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "irrelevance_67", "result": [{"hypothesis_testing.get_p_value": {"successes": 50, "n": 500, "prob_null": 0.1}}], "input_token_count": 780, "output_token_count": 44, "latency": 29.684544324874878}
{"id": "irrelevance_68", "result": [{"statistics.calculate_p_value": {"sample_mean": 98.2, "population_mean": 98.6, "sample_std_dev": 1.4, "sample_size": 40}}], "input_token_count": 730, "output_token_count": 54, "latency": 35.01721525192261}
{"id": "irrelevance_69", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "irrelevance_70", "result": [{"calculate_mortgage_payment": {"loan_amount": 10000, "loan_term": 3, "annual_interest_rate": 5}}], "input_token_count": 615, "output_token_count": 44, "latency": 30.214943408966064}
{"id": "irrelevance_71", "result": [{"calculate_ROI": {"investment_amount": 150000, "net_profit": 20000}}], "input_token_count": 599, "output_token_count": 40, "latency": 28.845160007476807}
{"id": "irrelevance_72", "result": [{"calculate_internal_rate_of_return": {"cash_flows": [100, -40, 60, 80, 120]}}], "input_token_count": 585, "output_token_count": 45, "latency": 30.24567484855652}
{"id": "irrelevance_73", "result": [{"finance.predict_revenue": {"company_name": "XYZ", "period": "next year"}}], "input_token_count": 594, "output_token_count": 30, "latency": 23.252832174301147}
{"id": "irrelevance_74", "result": [{"investment_analysis.calculate_profit": {"total_revenue": 15000, "total_cost": 22000}}], "input_token_count": 594, "output_token_count": 42, "latency": 29.18397307395935}
{"id": "irrelevance_75", "result": "[portfolio.returns(initial_investment=1, ending_value=1.5)]", "input_token_count": 580, "output_token_count": 19, "latency": 20.315979719161987}
{"id": "irrelevance_76", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "irrelevance_77", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "irrelevance_78", "result": "I'm sorry, but I don't have real-time data access. However, I can help you calculate the bond price if you provide the necessary details.", "input_token_count": 646, "output_token_count": 32, "latency": 24.63250184059143}
{"id": "irrelevance_79", "result": [{"stock_market_prediction": {"stock_name": "GOOGL", "days": 30}}], "input_token_count": 568, "output_token_count": 32, "latency": 25.163419485092163}
{"id": "irrelevance_80", "result": "[stock_ticker(company_name=\"FIFA\")]", "input_token_count": 624, "output_token_count": 12, "latency": 17.728028297424316}
{"id": "irrelevance_81", "result": "[get_stock_prices(ticker_symbol=\"AAPL\", start_date=\"2023-01-01\", end_date=\"2023-12-31\")]", "input_token_count": 601, "output_token_count": 43, "latency": 31.223337173461914}
{"id": "irrelevance_82", "result": "I'm sorry, but I don't have the ability to fetch real-time stock news. However, I can help you calculate capital gains or losses based on the purchase price, sale price, and number of shares.", "input_token_count": 636, "output_token_count": 44, "latency": 31.752134323120117}
{"id": "irrelevance_83", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "irrelevance_84", "result": "[get_stock_data(company_name=\"Lakers\", date=\"2023-10-01\"), get_stock_data(company_name=\"Celtics\", date=\"2023-10-01\")]", "input_token_count": 521, "output_token_count": 50, "latency": 35.47810101509094}
{"id": "irrelevance_85", "result": [{"criminal_case_details.get": {"case_number": "2020-001", "court_id": "2020-001"}}], "input_token_count": 587, "output_token_count": 44, "latency": 31.75423502922058}
{"id": "irrelevance_86", "result": [{"law_info.get_penalty": {"crime": "burglary", "state": "California"}}], "input_token_count": 514, "output_token_count": 32, "latency": 26.19502019882202}
{"id": "irrelevance_87", "result": [{"legal_case.file": {"court": "\u52a0\u5229\u798f\u5c3c\u4e9e\u5dde", "case_type": "\u5dde\u9577"}}], "input_token_count": 567, "output_token_count": 33, "latency": 26.74770951271057}
{"id": "irrelevance_88", "result": [{"detect_forgery": {"documents": ["2020\u5e74\u6700\u597d\u7684\u72af\u7f6a\u6050\u6016\u5206\u5b50\u96fb\u5f71\u662f\u4ec0\u9ebc\uff1f"], "machine_learning_model": "crime_horror_model"}}], "input_token_count": 615, "output_token_count": 54, "latency": 37.836912631988525}
{"id": "irrelevance_89", "result": [{"generate_contract": {"contract_type": "lease", "parties": ["tenant", "landlord"], "additional_details": {"state": "Texas"}}}], "input_token_count": 596, "output_token_count": 42, "latency": 31.708019495010376}
{"id": "irrelevance_90", "result": [{"file_complaint": {"complaint_type": "litter", "location": "Beijing", "details": "Noise complaint due to construction work."}}], "input_token_count": 593, "output_token_count": 44, "latency": 33.309903621673584}
{"id": "irrelevance_91", "result": [{"get_law_categories": {"law_type": "local", "country": "United States", "specific_category": "noise"}}], "input_token_count": 577, "output_token_count": 37, "latency": 29.090410947799683}
{"id": "irrelevance_92", "result": [{"search_lawyer": {"area": "New York", "specialization": "Security"}}], "input_token_count": 605, "output_token_count": 29, "latency": 25.63334012031555}
{"id": "irrelevance_93", "result": [{"law_firm.get_impactful_cases": {"firm_name": "xyz", "year": 2023}}], "input_token_count": 578, "output_token_count": 38, "latency": 30.10031032562256}
{"id": "irrelevance_94", "result": "[case_info.get(case_id=\"12345\", case_year=\"2020\")]", "input_token_count": 578, "output_token_count": 24, "latency": 23.153398036956787}
{"id": "irrelevance_95", "result": [{"case_review.retrieve_case_outcome": {"case_name": "Smith v. Doe", "case_year": 2023}}], "input_token_count": 592, "output_token_count": 39, "latency": 31.167160272598267}
{"id": "irrelevance_96", "result": [{"get_case_result": {"case_year": 1889, "case_name": "Eiffel Tower"}}], "input_token_count": 591, "output_token_count": 34, "latency": 29.072031497955322}
{"id": "irrelevance_97", "result": [{"file_lawsuit": {"defendant": "\u7d10\u7d04", "plaintiff": "\u60a8", "jurisdiction": "New York, NY"}}], "input_token_count": 588, "output_token_count": 44, "latency": 33.812774658203125}
{"id": "irrelevance_98", "result": "[lawsuit.settlement_estimate(damage_amount=100000, incident_type=\"property_damage\")]", "input_token_count": 583, "output_token_count": 29, "latency": 26.132032871246338}
{"id": "irrelevance_99", "result": "[lawsuit_search(location=\"\u5fb7\u514b\u85a9\u65af\u5dde\", subject=\"\u4ea4\u901a\u6cd5\")]", "input_token_count": 571, "output_token_count": 21, "latency": 22.717496156692505}
{"id": "irrelevance_100", "result": [{"calculate_litigation_cost": {"length_in_days": 10, "complexity": "low"}}], "input_token_count": 611, "output_token_count": 32, "latency": 19.13267469406128}
{"id": "irrelevance_101", "result": "[get_average_monthly_temperature(location=\"\u590f\u5a01\u5937\", month=\"12\")]", "input_token_count": 523, "output_token_count": 22, "latency": 13.440486907958984}
{"id": "irrelevance_102", "result": [{"calculate_sunrise_and_sunset": {"location": "New York, NY", "date": "2023-10-01"}}], "input_token_count": 619, "output_token_count": 42, "latency": 23.07392430305481}
{"id": "irrelevance_103", "result": [{"weather_forecast.get": {"location": "\u7d10\u7d04\u5e02"}}], "input_token_count": 543, "output_token_count": 24, "latency": 14.32612919807434}
{"id": "irrelevance_104", "result": [{"calculate_park_area": {"radius": 3, "units": "units", "shape": "sphere"}}], "input_token_count": 564, "output_token_count": 34, "latency": 18.786564350128174}
{"id": "irrelevance_105", "result": "[plot_elevation(start_point=\"\u5357\u7f8e\", end_point=\"\u6388\u7c89\", resolution=\"High\")]", "input_token_count": 577, "output_token_count": 23, "latency": 13.267203569412231}
{"id": "irrelevance_106", "result": "[soil_analysis.analyze_soil_type(soil_type=\"loam\", parameters_needed=[\"pH level\", \"Mineral content\", \"Organic matter content\"])]", "input_token_count": 620, "output_token_count": 42, "latency": 23.07991051673889}
{"id": "irrelevance_107", "result": [{"soil_composition_analyze": {"location": "\u6ce2\u58eb\u9813", "soil_sample": true}}], "input_token_count": 612, "output_token_count": 33, "latency": 16.68374538421631}
{"id": "irrelevance_108", "result": [{"emission_estimator": {"current_emissions": 10000, "action": "plant trees", "duration": 10}}], "input_token_count": 677, "output_token_count": 42, "latency": 20.96523404121399}
{"id": "irrelevance_109", "result": [{"calculate_water_needs": {"plant_type": "cactus", "location": "Arizona", "season": "summer"}}], "input_token_count": 640, "output_token_count": 38, "latency": 18.840860843658447}
{"id": "irrelevance_110", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "irrelevance_111", "result": [{"geo_location_based_products.fetch_eco_friendly_products": {"location": "Seattle, WA", "product_category": "Yoga Mats", "availability": "Instore"}}], "input_token_count": 662, "output_token_count": 47, "latency": 22.38460612297058}
{"id": "irrelevance_112", "result": "[geocode_address(address=\"New York\")]", "input_token_count": 555, "output_token_count": 11, "latency": 5.239062786102295}
{"id": "irrelevance_113", "result": "[find_pois(location=\"London\", category=[\"Restaurants\"])]", "input_token_count": 609, "output_token_count": 16, "latency": 7.154846668243408}
{"id": "irrelevance_114", "result": "[get_closest_airport(location=\"Los Angeles\", limit=1), get_closest_airport(location=\"New York\", limit=1)]", "input_token_count": 593, "output_token_count": 32, "latency": 14.573887825012207}
{"id": "irrelevance_115", "result": [{"calculate_distance": {"origin": {"latitude": 42.3601, "longitude": -71.0589}, "destination": {"latitude": 40.7128, "longitude": -74.006}, "speed": 60}}], "input_token_count": 553, "output_token_count": 74, "latency": 36.689995765686035}
{"id": "irrelevance_116", "result": [{"word_count": {"text": "\u60a8\u80fd\u63a8\u8350\u4e00\u90e8\u597d\u96fb\u5f71\u770b\u55ce\uff1f", "language": "zh"}}], "input_token_count": 502, "output_token_count": 36, "latency": 16.695369720458984}
{"id": "irrelevance_117", "result": [{"distance.calculate": {"from_lat": 34.0522, "from_long": -118.2437, "to_lat": 40.7128, "to_long": -74.006}}, {"distance.calculate": {"from_lat": 34.0522, "from_long": -118.2437, "to_lat": 36.1699, "to_long": -115.1398}}, {"distance.calculate": {"from_lat": 34.0522, "from_long": -118.2437, "to_lat": 41.8781, "to_long": -87.6298}}], "input_token_count": 667, "output_token_count": 193, "latency": 80.26323437690735}
{"id": "irrelevance_118", "result": "[playoff.brackets(year=1996, round=\"Finals\")]", "input_token_count": 536, "output_token_count": 20, "latency": 8.49045991897583}
{"id": "irrelevance_119", "result": [{"sentiment_analysis": {"text": "\u95dc\u65bc\u751f\u6d3b\u7684\u8457\u540d\u5831\u50f9", "model": "sentiment_analysis_model"}}], "input_token_count": 542, "output_token_count": 39, "latency": 18.312251091003418}
{"id": "irrelevance_120", "result": [{"caffeine_effect": {"caffeine_content": 100, "drinking_frequency": "daily", "drinking_duration": 100}}], "input_token_count": 613, "output_token_count": 46, "latency": 21.666666746139526}
{"id": "irrelevance_121", "result": "[medical_records.get_disease_info(disease_name=\"\u904b\u52d5\u795e\u7d93\u5143\u75be\u75c5\")]", "input_token_count": 519, "output_token_count": 22, "latency": 10.075534582138062}
{"id": "irrelevance_122", "result": "I'm sorry, but I don't have the function to get the average weight of the human brain.", "input_token_count": 649, "output_token_count": 22, "latency": 10.098552227020264}
{"id": "irrelevance_123", "result": [{"calculate_bmi": {"height": 1.8, "weight": 100}}], "input_token_count": 587, "output_token_count": 30, "latency": 13.485004663467407}
{"id": "irrelevance_124", "result": "[get_social_trends(category=\"technology\", region=\"worldwide\")]", "input_token_count": 509, "output_token_count": 18, "latency": 7.427662134170532}
{"id": "irrelevance_125", "result": "[get_recent_tweets(username=\"JKRowling\", count=5)]", "input_token_count": 552, "output_token_count": 17, "latency": 6.367556810379028}
{"id": "irrelevance_126", "result": "[get_happiness_index(country=\"United States\", year=2023, demographic_group=\"total\")]", "input_token_count": 657, "output_token_count": 26, "latency": 11.403512716293335}
{"id": "irrelevance_127", "result": [{"sentiment_analysis.twitter": {"topic": "\u65b0iPhone\u7248\u672c", "language": "zh"}}], "input_token_count": 564, "output_token_count": 32, "latency": 14.026510953903198}
{"id": "irrelevance_128", "result": "[personality_assessment.calculate_score(user_responses=[1, 2, 3, 4, 5])]", "input_token_count": 601, "output_token_count": 27, "latency": 11.364108324050903}
{"id": "irrelevance_129", "result": [{"personality_assessment.evaluate": {"traits": [{"trait": "extraversion", "percentage": 70}, {"trait": "introversion", "percentage": 30}, {"trait": "sensing", "percentage": 50}, {"trait": "intuition", "percentage": 50}, {"trait": "thinking", "percentage": 60}, {"trait": "feeling", "percentage": 40}, {"trait": "judging", "percentage": 50}, {"trait": "perceiving", "percentage": 50}]}}], "input_token_count": 664, "output_token_count": 144, "latency": 59.40586829185486}
{"id": "irrelevance_130", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "irrelevance_131", "result": [{"psychology.color_representation": {"color": "\u7d2b\u8272", "context": "\u8a08\u7b97\u6a5f\u8996\u89ba"}}], "input_token_count": 592, "output_token_count": 35, "latency": 16.14512276649475}
{"id": "irrelevance_132", "result": "[historical_event.get_date(event_name=\"\u6ed1\u9435\u76e7\u6230\u5f79\", format=\"YYYY-MM-DD\")]", "input_token_count": 509, "output_token_count": 27, "latency": 11.864978551864624}
{"id": "irrelevance_133", "result": "[get_battle_details(battle_name=\"NBA Finals\", year=2023)]", "input_token_count": 579, "output_token_count": 20, "latency": 8.613072395324707}
{"id": "irrelevance_134", "result": "[calculate_battle_outcome(battle_name=\"2022\u5e74\u4e16\u754c\u676f\", strategy_type=\"unknown\")]", "input_token_count": 589, "output_token_count": 25, "latency": 11.138612508773804}
{"id": "irrelevance_135", "result": [{"add_dates": {"date": "1776-07-04", "days_to_add": 0}}], "input_token_count": 556, "output_token_count": 37, "latency": 17.588382482528687}
{"id": "irrelevance_136", "result": [{"us_president_in_year": {"year": 2023}}], "input_token_count": 500, "output_token_count": 25, "latency": 11.455266237258911}
{"id": "irrelevance_137", "result": "[historical_event.get_date(event_name=\"Declaration of Independence\", event_location=\"United States\")]", "input_token_count": 594, "output_token_count": 21, "latency": 9.642353773117065}
{"id": "irrelevance_138", "result": [{"calculate_age": {"birthdate": "1776-07-04", "current_date": "2023-10-05"}}], "input_token_count": 534, "output_token_count": 45, "latency": 22.052425861358643}
{"id": "irrelevance_139", "result": [{"space.star_info": {"star_name": "Sol", "information": "mass"}}], "input_token_count": 527, "output_token_count": 28, "latency": 13.378551006317139}
{"id": "irrelevance_140", "result": [{"calculate_electric_current": {"voltage": 12, "resistance": 1}}], "input_token_count": 573, "output_token_count": 29, "latency": 14.127658367156982}
{"id": "irrelevance_141", "result": [{"look_up_scientific_contributions": {"scientist_name": "Hydrogen", "contributions": 3}}], "input_token_count": 535, "output_token_count": 35, "latency": 17.027093648910522}
{"id": "irrelevance_142", "result": "[get_element_properties(element=\"Einstein\")]", "input_token_count": 460, "output_token_count": 11, "latency": 4.842742443084717}
{"id": "irrelevance_143", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "irrelevance_144", "result": [{"search_holy_books": {"book": "\u8056\u66f8"}}], "input_token_count": 551, "output_token_count": 24, "latency": 11.556900024414062}
{"id": "irrelevance_145", "result": [{"religion_history.get_event_year": {"event_name": "Protestant Reformation", "period": "16th century"}}], "input_token_count": 562, "output_token_count": 36, "latency": 17.596129417419434}
{"id": "irrelevance_146", "result": [{"get_prophet_details": {"religion": "Islam", "prophet": "Muhammad", "historical_context": true}}], "input_token_count": 603, "output_token_count": 38, "latency": 18.62050700187683}
{"id": "irrelevance_147", "result": "[color_mix.mix_two_colors(color1=\"#FAEBD7\", color2=\"#00FFFF\")]", "input_token_count": 622, "output_token_count": 25, "latency": 12.237123727798462}
{"id": "irrelevance_148", "result": [{"color_complimentary": {"color": "blue"}}], "input_token_count": 561, "output_token_count": 22, "latency": 10.62446403503418}
{"id": "irrelevance_149", "result": [{"calculate_paint_mix": {"target_color": "\u5929\u85cd\u8272", "available_colors": ["\u85cd\u8272", "\u767d\u8272", "\u9ed1\u8272"]}}], "input_token_count": 609, "output_token_count": 44, "latency": 22.503737688064575}
{"id": "irrelevance_150", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "irrelevance_151", "result": "[sculpture.get_dimensions(sculpture_name=\"\u7562\u52a0\u7d22\u756b\", artist_name=\"\u7562\u52a0\u7d22\")]", "input_token_count": 571, "output_token_count": 29, "latency": 15.054023742675781}
{"id": "irrelevance_152", "result": [{"sculpture.create": {"design": "\u82b1\u5712\u96d5\u5851", "material": "\u82b1\u5712\u96d5\u5851\u6750\u6599", "size": "\u82b1\u5712\u96d5\u5851\u5927\u5c0f"}}], "input_token_count": 561, "output_token_count": 53, "latency": 27.835495948791504}
{"id": "irrelevance_153", "result": "[material_tool_lookup.lookup(material=\"stone\", sculpting_technique=\"carving\")]", "input_token_count": 619, "output_token_count": 23, "latency": 12.274244785308838}
{"id": "irrelevance_154", "result": [{"sculpture_info.find_creator": {"sculpture_name": "Nou Camp Stadium", "location": "Nou Camp Stadium"}}], "input_token_count": 592, "output_token_count": 39, "latency": 20.427797555923462}
{"id": "irrelevance_155", "result": "[architecture_capacity.evaluate_capacity(structure_name=\"\u601d\u60f3\u5bb6\u96d5\u5851\", area_per_person=6)]", "input_token_count": 564, "output_token_count": 25, "latency": 13.341854095458984}
{"id": "irrelevance_156", "result": [{"generate_architecture_plan": {"style": "Gothic", "building_type": "Church", "extra_features": ["Pool", "Garage", "Garden", "Elevator"]}}], "input_token_count": 645, "output_token_count": 51, "latency": 27.42593240737915}
{"id": "irrelevance_157", "result": [{"building_information.get_data": {"building_name": "\u5927\u6559\u5802", "info_requested": "\u5929\u82b1\u677f"}}], "input_token_count": 536, "output_token_count": 35, "latency": 18.831084966659546}
{"id": "irrelevance_158", "result": [{"calculate_construction_cost": {"building_type": "house", "location": "New York", "materials": ["brick", "concrete", "steel"], "labor_cost": 100}}], "input_token_count": 654, "output_token_count": 54, "latency": 29.026840209960938}
{"id": "irrelevance_159", "result": [{"artwork_search": {"artwork_name": "\u5c16\u53eb", "museum_location": "Paris, France"}}], "input_token_count": 587, "output_token_count": 33, "latency": 17.95977282524109}
{"id": "irrelevance_160", "result": [{"most_frequent_visitor": {"museum_name": "\u73fe\u4ee3\u85dd\u8853\u535a\u7269\u9928", "start_date": "2022-01-01", "end_date": "2022-12-31"}}], "input_token_count": 668, "output_token_count": 63, "latency": 33.51886868476868}
{"id": "irrelevance_161", "result": [{"museum_data.get_visit_stats": {"city": "New York", "year": 2023}}], "input_token_count": 570, "output_token_count": 33, "latency": 17.979540824890137}
{"id": "irrelevance_162", "result": "[get_museum_artists(museum_name=\"The Metropolitan Museum of Art\", period=\"19th Century\", country=\"USA\")]", "input_token_count": 599, "output_token_count": 30, "latency": 15.78451156616211}
{"id": "irrelevance_163", "result": "[tune_instrument(instrument_type='acoustic guitar', key='Standard')]", "input_token_count": 560, "output_token_count": 18, "latency": 10.312544584274292}
{"id": "irrelevance_164", "result": "[search_music_instrument_players(instrument=\"vocal\", genre=\"Jazz\", top=5)]", "input_token_count": 573, "output_token_count": 24, "latency": 13.319817543029785}
{"id": "irrelevance_165", "result": "[get_instrument_info(instrument_name=\"\u5927\u63d0\u7434\", detail=\"type\")]", "input_token_count": 543, "output_token_count": 20, "latency": 10.874560356140137}
{"id": "irrelevance_166", "result": "I'm sorry, but I don't have any functions that can provide information on maintaining a piano.", "input_token_count": 569, "output_token_count": 21, "latency": 11.948465824127197}
{"id": "irrelevance_167", "result": "[get_concert_info(concert_id=123, include_artist_info=true)]", "input_token_count": 561, "output_token_count": 20, "latency": 11.407991170883179}
{"id": "irrelevance_168", "result": "[concert_availability(artist=\"\u5927\u5b78\", location=\"\u6089\u5c3c\", date=\"2023-05\")]", "input_token_count": 560, "output_token_count": 29, "latency": 15.263435363769531}
{"id": "irrelevance_169", "result": [{"concert_search.find_concerts": {"venue": "\u9ea5\u8fea\u905c\u5ee3\u5834\u82b1\u5712", "date": "2023-10-05"}}], "input_token_count": 618, "output_token_count": 47, "latency": 24.929373025894165}
{"id": "irrelevance_170", "result": "[music_theory.create_chord_progression(key=\"C\", progression_pattern=[\"C\", \"G\", \"Am\", \"F\"])]", "input_token_count": 524, "output_token_count": 29, "latency": 15.23696517944336}
{"id": "irrelevance_171", "result": [{"music.search_composer": {"title": "The Laws", "epoch": "Ancient Greece"}}], "input_token_count": 551, "output_token_count": 30, "latency": 16.31524348258972}
{"id": "irrelevance_172", "result": "[music_composer.composition_info(composition_name=\"Don Quixote\", need_detailed_info=True)]", "input_token_count": 556, "output_token_count": 23, "latency": 12.673364400863647}
{"id": "irrelevance_173", "result": [{"music_analysis.find_common_chords": {"genre": "C\u5c08\u696d\u9470\u5319", "num_chords": 3}}], "input_token_count": 528, "output_token_count": 39, "latency": 21.176676750183105}
{"id": "irrelevance_174", "result": [{"music_theory.primary_triads": {"key_signature": "C", "include_inversions": true}}], "input_token_count": 540, "output_token_count": 33, "latency": 17.959983348846436}
{"id": "irrelevance_175", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "irrelevance_176", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "irrelevance_177", "result": "[get_song_chord_progression(song_name=\"2017\u5e74\u7372\u5f97\u4e86\u683c\u840a\u7f8e\u734e\u6700\u4f73\u5c08\u8f2f\u734e\", artist_name=\"2017\u5e74\u7372\u5f97\u4e86\u683c\u840a\u7f8e\u734e\u6700\u4f73\u5c08\u8f2f\u734e\")]", "input_token_count": 585, "output_token_count": 51, "latency": 27.675626039505005}
{"id": "irrelevance_178", "result": "[sports_analysis.get_top_scorer(league=\"Premier League\", season=\"2020/2021\")]", "input_token_count": 609, "output_token_count": 31, "latency": 16.887743949890137}
{"id": "irrelevance_179", "result": "[get_game_results(team_1=\"Lakers\", team_2=\"Warriors\", date=\"2023-10-10\")]", "input_token_count": 617, "output_token_count": 34, "latency": 17.957391500473022}
{"id": "irrelevance_180", "result": "[sports_analyzer.get_schedule(date=\"2023-10-05\", sport=\"cricket\")]", "input_token_count": 595, "output_token_count": 28, "latency": 14.73068618774414}
{"id": "irrelevance_181", "result": "[soccer_stats.get_last_match_result(team1=\"Real Madrid\", team2=\"FC Barcelona\")]", "input_token_count": 565, "output_token_count": 23, "latency": 12.636827230453491}
{"id": "irrelevance_182", "result": "[get_nba_player_stats(player_name=\"\u9081\u514b\u723e\u00b7\u55ac\u4e39\", stat_type=\"championships\")]", "input_token_count": 601, "output_token_count": 27, "latency": 14.706749439239502}
{"id": "irrelevance_183", "result": "[find_top_sports_celebrity(name=\"Novak Djokovic\", year=2021, sports_type=\"Tennis\")]", "input_token_count": 620, "output_token_count": 33, "latency": 17.923527479171753}
{"id": "irrelevance_184", "result": "[sports_stats.get_player_stats(player_name=\"LeBron James\", season=\"2020-2021\")]", "input_token_count": 615, "output_token_count": 27, "latency": 14.79176378250122}
{"id": "irrelevance_185", "result": "[player_stats.average_scoring(player_name=\"\u52d2\u5e03\u6717\u00b7\u8a79\u59c6\u65af\", season=\"2022-2023\")]", "input_token_count": 569, "output_token_count": 30, "latency": 16.3610098361969}
{"id": "irrelevance_186", "result": [{"sports_ranking.get_MVP": {"season": "2023", "sport_type": "soccer"}}], "input_token_count": 561, "output_token_count": 37, "latency": 19.588169813156128}
{"id": "irrelevance_187", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "irrelevance_188", "result": "[sports.ranking.get_champion(event=\"World Series\", year=2020)]", "input_token_count": 506, "output_token_count": 21, "latency": 11.060412883758545}
{"id": "irrelevance_189", "result": [{"sports_ranking.get_top_ranked": {"sport": "basketball", "gender": "male"}}], "input_token_count": 560, "output_token_count": 34, "latency": 18.06774067878723}
{"id": "irrelevance_190", "result": [{"sports_team.standing": {"team_name": "Roger Federer", "league": "ATP"}}], "input_token_count": 611, "output_token_count": 31, "latency": 16.451392650604248}
{"id": "irrelevance_191", "result": "[get_match_stats(team_name=\"Germany\", tournament=\"World Cup\", year=2014)]", "input_token_count": 578, "output_token_count": 23, "latency": 12.145940065383911}
{"id": "irrelevance_192", "result": [{"sports_team.get_top_scorer": {"team": "\u66fc\u806f", "season": "2020-2021", "league": "\u82f1\u8d85"}}], "input_token_count": 593, "output_token_count": 48, "latency": 25.8466477394104}
{"id": "irrelevance_193", "result": "[get_sport_team_details(team_name=\"Lakers\", details=[\"roster\"])]", "input_token_count": 573, "output_token_count": 20, "latency": 10.762197732925415}
{"id": "irrelevance_194", "result": "[fetch_game_stats(game_type=\"chess\", year=2023, location=\"NY\")]", "input_token_count": 582, "output_token_count": 23, "latency": 12.902987480163574}
{"id": "irrelevance_195", "result": [{"game.board_analyser": {"game": "chess", "player": "white", "position": "rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1"}}], "input_token_count": 691, "output_token_count": 73, "latency": 37.80457305908203}
{"id": "irrelevance_196", "result": "[boardgame.calculate_score(player_actions=[{'player_id': 1, 'action': 'buy property', 'property_id': 1}, {'player_id': 2, 'action': 'sell property', 'property_id': 2}, {'player_id': 3, 'action': 'pass go'}, {'player_id': 4, 'action': 'pay fine'}], initial_scores={'player_id': 1, 'score': 0, 'player_id': 2, 'score': 0, 'player_id': 3, 'score': 0, 'player_id': 4, 'score': 0})]", "input_token_count": 849, "output_token_count": 136, "latency": 58.49544882774353}
{"id": "irrelevance_197", "result": "[board_game.possible_moves(game_name=\"\u58df\u65b7\u904a\u6232\", current_state=\"current_state\")]", "input_token_count": 603, "output_token_count": 26, "latency": 14.554903745651245}
{"id": "irrelevance_198", "result": [{"cards.shuffle_deck": {"deck": "Uno", "times": 1}}], "input_token_count": 573, "output_token_count": 28, "latency": 15.11318325996399}
{"id": "irrelevance_199", "result": "[play_poker(number_of_players=2, cards_per_player=5)]", "input_token_count": 571, "output_token_count": 18, "latency": 9.700750589370728}
{"id": "irrelevance_200", "result": [{"get_highest_card_holder": {"game_id": "21", "suit": "hearts"}}], "input_token_count": 545, "output_token_count": 32, "latency": 17.758800268173218}
{"id": "irrelevance_201", "result": [{"game_guide": {"game_name": "\u51b0\u6dc7\u6dcb\u5e97", "level": 1}}], "input_token_count": 603, "output_token_count": 33, "latency": 18.32374095916748}
{"id": "irrelevance_202", "result": "[game_score.calculate(team1_points=100, team2_points=90)]", "input_token_count": 597, "output_token_count": 21, "latency": 11.869133234024048}
{"id": "irrelevance_203", "result": "[get_player_score(player=\"A\", game=\"\u904a\u6232\u5149\u74b0\")]", "input_token_count": 494, "output_token_count": 18, "latency": 10.251626968383789}
{"id": "irrelevance_204", "result": [{"game_functions.solve_jigsaw": {"puzzle_image": "puzzle_image.jpg", "pieces_count": 100}}], "input_token_count": 634, "output_token_count": 38, "latency": 20.473649501800537}
{"id": "irrelevance_205", "result": [{"calculate_score": {"enemies_defeated": 10, "coins_collected": 50, "power_ups": 3}}], "input_token_count": 617, "output_token_count": 42, "latency": 23.706353902816772}
{"id": "irrelevance_206", "result": "[game.find_best_weapon(player_level=10, enemy_type='dragon', inventory=['sword', 'bow', 'knife'])]", "input_token_count": 630, "output_token_count": 31, "latency": 17.737810611724854}
{"id": "irrelevance_207", "result": "[game_tracker.high_score(game_name=\"\u8106\u5f31\u7684\u9ce5\u985e\", platform=\"PC\")]", "input_token_count": 599, "output_token_count": 22, "latency": 12.892316102981567}
{"id": "irrelevance_208", "result": "[calculate_taxi_fare(distance=10.5, wait_time=5)]", "input_token_count": 558, "output_token_count": 21, "latency": 11.885337114334106}
{"id": "irrelevance_209", "result": [{"fetch_recipe": {"query": "how to build a computer", "numberOfResults": 1}}], "input_token_count": 598, "output_token_count": 32, "latency": 17.79855227470398}
{"id": "irrelevance_210", "result": [{"recipe_based_restaurants": {"recipe_name": "French cuisine", "location": "Paris"}}], "input_token_count": 656, "output_token_count": 31, "latency": 17.79851245880127}
{"id": "irrelevance_211", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "irrelevance_212", "result": "[get_cooking_time(ingredient_type=\"egg\", ingredient_size=\"large\")]", "input_token_count": 615, "output_token_count": 18, "latency": 10.280686140060425}
{"id": "irrelevance_213", "result": "[restaurant_finder(cuisine=\"pizza\", location=\"\u6ce2\u58eb\u9813\")]", "input_token_count": 560, "output_token_count": 19, "latency": 10.807282447814941}
{"id": "irrelevance_214", "result": "I'm sorry, but I don't have the ability to find restaurants.", "input_token_count": 562, "output_token_count": 16, "latency": 9.194523096084595}
{"id": "irrelevance_215", "result": "I'm sorry, but I don't have the ability to calculate travel times.", "input_token_count": 578, "output_token_count": 17, "latency": 10.281274557113647}
{"id": "irrelevance_216", "result": "[convert_currency(amount=100, from_currency=\"USD\", to_currency=\"EUR\")]", "input_token_count": 544, "output_token_count": 22, "latency": 12.431883096694946}
{"id": "irrelevance_217", "result": "Sure, I can help with that. Could you please provide the ingredients and cooking instructions for the recipe?", "input_token_count": 600, "output_token_count": 22, "latency": 12.427058935165405}
{"id": "irrelevance_218", "result": [{"prepare_food.get_recipe": {"ingredients": ["\u5de7\u514b\u529b", "\u86cb", "\u5976\u6cb9", "\u7cd6", "\u9eb5\u7c89", "\u70e4\u7bb1"], "food_type": "\u86cb\u7cd5"}}], "input_token_count": 601, "output_token_count": 60, "latency": 31.99452567100525}
{"id": "irrelevance_219", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "irrelevance_220", "result": [{"recipe.getTemperature": {"dish_name": "\u5de7\u514b\u529b\u86cb\u7cd5", "oven_type": "Convection"}}], "input_token_count": 581, "output_token_count": 37, "latency": 21.054051876068115}
{"id": "irrelevance_221", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "irrelevance_222", "result": [{"grocery_store.item_details": {"item_name": "\u756a\u8304", "store_location": "\u5317\u4eac", "details_level": "detailed"}}], "input_token_count": 621, "output_token_count": 41, "latency": 22.716914176940918}
{"id": "irrelevance_223", "result": "[grocery_shop.find_specific_product(city=\"\u829d\u52a0\u54e5\", product=\"\u9178\u9eb5\u5305\")]", "input_token_count": 601, "output_token_count": 28, "latency": 15.140411853790283}
{"id": "irrelevance_224", "result": "[grocery_store.locate_nearby(location=\"Los Angeles, CA\", store_type=[\"Pet Store\"], is_24_hours=True)]", "input_token_count": 652, "output_token_count": 32, "latency": 17.831833362579346}
{"id": "irrelevance_225", "result": "[time_converter(user_timezone='Eastern Time (US & Canada)', target_timezone='Eastern Time (US & Canada)')]", "input_token_count": 702, "output_token_count": 31, "latency": 16.746310710906982}
{"id": "irrelevance_226", "result": "[get_local_time(timezone=\"Europe/London\", date_format=\"YYYY-MM-DD HH:mm:ss\")]", "input_token_count": 539, "output_token_count": 23, "latency": 12.976231098175049}
{"id": "irrelevance_227", "result": [{"calculate_sunrise": {"location": "\u5317\u4eac", "date": "2023-10-03"}}], "input_token_count": 643, "output_token_count": 36, "latency": 19.46878457069397}
{"id": "irrelevance_228", "result": "[get_local_time(location=\"Sydney\")]", "input_token_count": 623, "output_token_count": 10, "latency": 5.456017971038818}
{"id": "irrelevance_229", "result": "I'm sorry, but I don't have information about restaurants in Tokyo.", "input_token_count": 651, "output_token_count": 16, "latency": 8.681211948394775}
{"id": "irrelevance_230", "result": "[find_hotel(location=\"\u8fc8\u963f\u5bc6\", max_price_per_night=100, pet_friendly=True)]", "input_token_count": 590, "output_token_count": 26, "latency": 14.597327709197998}
{"id": "irrelevance_231", "result": "[hotel_booking.check_availability(hotel_name=\"\u6cf0\u570b\u9910\u5ef3\", location=\"\u829d\u52a0\u54e5\", check_in_date=\"2023-10-01\", check_out_date=\"2023-10-02\", room_type=\"double\")]", "input_token_count": 686, "output_token_count": 63, "latency": 32.52806854248047}
{"id": "irrelevance_232", "result": "[hotel_search.find_hotels(location=\"New York, NY\", amenities=[\"Breakfast\", \"Fitness Centre\", \"Free Wi-Fi\", \"Parking\"])", "input_token_count": 588, "output_token_count": 35, "latency": 19.454065799713135}
{"id": "irrelevance_233", "result": [{"weather_in_location": {"location": "London", "unit": "F"}}], "input_token_count": 544, "output_token_count": 26, "latency": 14.07000470161438}
{"id": "irrelevance_234", "result": [{"convert_currency": {"amount": 10, "from_currency": "inches", "to_currency": "centimeters"}}], "input_token_count": 551, "output_token_count": 37, "latency": 20.566205978393555}
{"id": "irrelevance_235", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "irrelevance_236", "result": "[get_flight_duration(start_city=\"London\", destination_city=\"Tokyo\", flight_type=\"non-stop\")]", "input_token_count": 601, "output_token_count": 23, "latency": 12.972686052322388}
{"id": "irrelevance_237", "result": "[get_route_to_location(start_point=\"Los Angeles\", end_point=\"Pharmacy\")]", "input_token_count": 610, "output_token_count": 20, "latency": 10.837607145309448}
{"id": "irrelevance_238", "result": [{"map_coordinates.distance_calculate": {"pointA": {"latitude": 5, "longitude": 6}, "pointB": {"latitude": 5, "longitude": 6}}}], "input_token_count": 773, "output_token_count": 49, "latency": 26.00215244293213}
{"id": "irrelevance_239", "result": "[get_date(location_1=\"\u65e7\u91d1\u5c71\", location_2=\"\u6d1b\u6749\u77f6\", unit=\"kilometers\")]", "input_token_count": 580, "output_token_count": 27, "latency": 14.610463857650757}
