{"id": "multiple_0", "result": "[triangle_properties.get(side1=5, side2=4, side3=3, get_area=True, get_perimeter=True, get_angles=True)]", "input_token_count": 1243, "output_token_count": 35, "latency": 253.84204745292664}
{"id": "multiple_1", "result": "[math.triangle_area_heron(side1=3, side2=4, side3=5)]", "input_token_count": 1072, "output_token_count": 25, "latency": 246.13225507736206}
{"id": "multiple_2", "result": "[country_info.capital(country=\"\u5df4\u897f\")]", "input_token_count": 817, "output_token_count": 13, "latency": 237.6282241344452}
{"id": "multiple_3", "result": "[EuclideanDistance.calculate(pointA=[3, 4], pointB=[1, 2])]", "input_token_count": 956, "output_token_count": 23, "latency": 250.83935451507568}
{"id": "multiple_4", "result": "[kinematics.calculate_displacement(initial_speed=20, acceleration=10, time=5)]", "input_token_count": 1131, "output_token_count": 23, "latency": 257.8419907093048}
{"id": "multiple_5", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "multiple_6", "result": "[capacitance_calculator.calculate(A=10, d=0.01, K=1.0)]", "input_token_count": 1166, "output_token_count": 27, "latency": 257.9790234565735}
{"id": "multiple_7", "result": "[wildlife_population.assess_growth(species=\"deer\", location=\"Washington\", duration=10), ecological_impact.analyze(species=\"deer\", ecosystem=\"forest\", location=\"Washington\", timeframe=10)]", "input_token_count": 988, "output_token_count": 54, "latency": 273.24439215660095}
{"id": "multiple_8", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "multiple_9", "result": "[calculate_average(gradeDict={'\u6578\u5b78': 90, '\u79d1\u5b78': 75, '\u6b77\u53f2': 82, '\u97f3\u6a02': 89})]", "input_token_count": 936, "output_token_count": 41, "latency": 274.31925344467163}
{"id": "multiple_10", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "multiple_11", "result": "[math_roots.quadratic(a=5, b=20, c=-25)]", "input_token_count": 1160, "output_token_count": 22, "latency": 259.07483768463135}
{"id": "multiple_12", "result": "[corporate_finance.calculate_YOY_growth_rate(company_name=\"Tech Inc\", year1=2019, year1_revenue=100000000, year2=2020, year2_revenue=120000000)]", "input_token_count": 1283, "output_token_count": 67, "latency": 279.7940375804901}
{"id": "multiple_13", "result": "[corporate_finance.revenue_forecast(company=\"XYZ\", product=\"Product1\", sales_units_increase_percentage=10)]", "input_token_count": 886, "output_token_count": 31, "latency": 262.47310757637024}
{"id": "multiple_14", "result": [{"finance.property_depreciation": {"initial_cost": 200000, "depreciation_rate": 3, "years": 5}}], "input_token_count": 1343, "output_token_count": 44, "latency": 270.00226402282715}
{"id": "multiple_15", "result": "[solarFarm.potential(coordinates=[37.7749, -122.4194], panelArea=80000, month=\"December\")]", "input_token_count": 1042, "output_token_count": 43, "latency": 272.27261877059937}
{"id": "multiple_16", "result": "[population_genetics.calculate_ne(species=\"\u8001\u864e\", generations=100, probability=0.9)]", "input_token_count": 1297, "output_token_count": 26, "latency": 262.13539385795593}
{"id": "multiple_17", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "multiple_18", "result": "[european_history.battle_details(battle=\"Stalingrad\")]", "input_token_count": 854, "output_token_count": 16, "latency": 251.37861561775208}
{"id": "multiple_19", "result": "[religion_history.get_schisms(religion=\"\u57fa\u7763\u6559\", count=3)]", "input_token_count": 1083, "output_token_count": 21, "latency": 258.4151060581207}
{"id": "multiple_20", "result": "[sculpture_price.calculate(material=\"marble\", size=3)]", "input_token_count": 1070, "output_token_count": 17, "latency": 250.69091653823853}
{"id": "multiple_21", "result": "[generate_sound_wave(frequency=440, duration=5)]", "input_token_count": 937, "output_token_count": 18, "latency": 249.6486234664917}
{"id": "multiple_22", "result": "[sports_data.basketball.most_points_single_game(league='NBA')]", "input_token_count": 1090, "output_token_count": 19, "latency": 248.3823277950287}
{"id": "multiple_23", "result": "[basketball.player_stats.get(player_name=\"Lebron James\", stats_fields=[\"points\", \"assists\", \"rebounds\", \"minutes\"])]", "input_token_count": 1272, "output_token_count": 32, "latency": 254.4984049797058}
{"id": "multiple_24", "result": "[route_planner.calculate_route(start=\"\u502b\u6566\", destination=\"\u611b\u4e01\u5821\", method=\"fastest\")]", "input_token_count": 930, "output_token_count": 24, "latency": 237.20975947380066}
{"id": "multiple_25", "result": "[video_games.store_price(game_title=\"\u523a\u5ba2\u4fe1\u689d\u74e6\u723e\u54c8\u62c9\", platform=\"PlayStation\")]", "input_token_count": 1267, "output_token_count": 26, "latency": 235.3970286846161}
{"id": "multiple_26", "result": "[game_rewards.get(game=\"Fortnite\", platform=\"PlayStation\")]", "input_token_count": 1250, "output_token_count": 18, "latency": 229.3759834766388}
{"id": "multiple_27", "result": [{"maps.shortest_path": {"start_location": "\u6cd5\u570b\u5df4\u9ece", "end_location": "\u610f\u5927\u5229\u7f85\u99ac"}}], "input_token_count": 889, "output_token_count": 33, "latency": 234.85747909545898}
{"id": "multiple_28", "result": "[solve.quadratic_equation(a=2, b=3, c=-4)]", "input_token_count": 1024, "output_token_count": 20, "latency": 222.78588247299194}
{"id": "multiple_29", "result": "[functions.intersect(function1=\"3x+2\", function2=\"2x+3\")]", "input_token_count": 777, "output_token_count": 22, "latency": 223.38292860984802}
{"id": "multiple_30", "result": "[rectangle.area(length=12, width=5)]", "input_token_count": 984, "output_token_count": 15, "latency": 224.1347587108612}
{"id": "multiple_31", "result": "[geometry_rectangle.calculate(width=7, length=10)]", "input_token_count": 930, "output_token_count": 15, "latency": 229.9734330177307}
{"id": "multiple_32", "result": "[geometry.calculate_cone_volume(radius=4, height=7)]", "input_token_count": 868, "output_token_count": 16, "latency": 226.76427745819092}
{"id": "multiple_33", "result": "[calculate_integral(func=\"3*x**2\", a=1, b=2)]", "input_token_count": 869, "output_token_count": 21, "latency": 226.22718286514282}
{"id": "multiple_34", "result": "[math.lcm(num1=18, num2=12)]", "input_token_count": 982, "output_token_count": 17, "latency": 222.06537985801697}
{"id": "multiple_35", "result": "[calculate_gcd(num1=128, num2=256)]", "input_token_count": 944, "output_token_count": 19, "latency": 214.1946268081665}
{"id": "multiple_36", "result": "[kinematics.calculate_speed_from_rest(distance=20, time=4)]", "input_token_count": 987, "output_token_count": 19, "latency": 213.54775047302246}
{"id": "multiple_37", "result": "[kinematics.final_velocity(initial_velocity=40, time=6)]", "input_token_count": 1216, "output_token_count": 20, "latency": 212.90864086151123}
{"id": "multiple_38", "result": "[library.search_book(book_name=\"\u7149\u91d1\u8853\u58eb\", city=\"\u7d10\u7d04\u5e02\")]", "input_token_count": 972, "output_token_count": 22, "latency": 218.80137968063354}
{"id": "multiple_39", "result": "[ride_hailing.get_rides(source=\"\u7d10\u7d04\", destination=\"\u8cbb\u57ce\", max_cost=50)]", "input_token_count": 911, "output_token_count": 25, "latency": 217.78769326210022}
{"id": "multiple_40", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "multiple_41", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "multiple_42", "result": "[calculate_final_temperature(quantity1=2, temperature1=300, quantity2=3, temperature2=400)]", "input_token_count": 911, "output_token_count": 31, "latency": 218.75204801559448}
{"id": "multiple_43", "result": "[biological.calc_energy(mols=5, substance='C6H12O6')]", "input_token_count": 1127, "output_token_count": 24, "latency": 214.76996517181396}
{"id": "multiple_44", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "multiple_45", "result": "[geology.get_era(era_name=\"Ice age\", calculate_years_ago=True)]", "input_token_count": 772, "output_token_count": 21, "latency": 207.06609892845154}
{"id": "multiple_46", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "multiple_47", "result": [{"cosine_similarity.calculate": {"vector1": [3, 2, 1], "vector2": [1, 2, 3]}}], "input_token_count": 996, "output_token_count": 44, "latency": 219.92208075523376}
{"id": "multiple_48", "result": "[library.find_nearby(location=\"New York City, NY\", preferences=[\"Pet-friendly\", \"Disabled Access\"])]", "input_token_count": 938, "output_token_count": 25, "latency": 208.12474870681763}
{"id": "multiple_49", "result": [{"calc_Compound_Interest": {"principle_amount": 1000, "duration": 2, "annual_rate": 2.5, "compound_freq": 1500}}], "input_token_count": 1327, "output_token_count": 55, "latency": 227.81531715393066}
{"id": "multiple_50", "result": "[house_price_forecast(location=\"New York\", months=1)]", "input_token_count": 1156, "output_token_count": 15, "latency": 203.3492157459259}
{"id": "multiple_51", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "multiple_52", "result": [{"currency_conversion": {"amount": 100, "from_currency": "EUR", "to_currency": "USD"}}], "input_token_count": 830, "output_token_count": 37, "latency": 214.0625216960907}
{"id": "multiple_53", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "multiple_54", "result": "[corporate_finance.dividend_data(company=\"Apple Inc.\", years=5)]", "input_token_count": 872, "output_token_count": 21, "latency": 199.01649284362793}
{"id": "multiple_55", "result": "[stock_forecast(company=\"Google\", days=3)]", "input_token_count": 831, "output_token_count": 14, "latency": 193.8361885547638}
{"id": "multiple_56", "result": "[avg_closing_price(company='apple', days=60)]", "input_token_count": 1198, "output_token_count": 16, "latency": 195.01743030548096}
{"id": "multiple_57", "result": "[financial.compound_interest(principle=1000, rate=0.05, time=10, n=4)]", "input_token_count": 1075, "output_token_count": 34, "latency": 207.481915473938}
{"id": "multiple_58", "result": "[lawyer.search(location=\"Los Angeles, CA\", expertise=\"Marriage\")]", "input_token_count": 826, "output_token_count": 18, "latency": 196.3718147277832}
{"id": "multiple_59", "result": "[lawyer_finder(location=\"New York\", specialization=[\"criminal law\"])]", "input_token_count": 899, "output_token_count": 18, "latency": 192.1580102443695}
{"id": "multiple_60", "result": [{"humidity_temperature_forecast": {"location": "\u7d10\u7d04\u5e02", "days": 7}}], "input_token_count": 1080, "output_token_count": 32, "latency": 198.03115224838257}
{"id": "multiple_61", "result": "[landscape_architect.find_specialty(location=\"Portland, OR\", specialization=\"residential design\", years_experience=5)]", "input_token_count": 1008, "output_token_count": 30, "latency": 196.8436188697815}
{"id": "multiple_62", "result": "[nature_park.find_nearby(location=\"Boston, MA\", features=[\"Camping\", \"Scenic View\"])]", "input_token_count": 920, "output_token_count": 26, "latency": 193.38673996925354}
{"id": "multiple_63", "result": [{"air_quality_forecast": {"location": "New York", "days": 7}}], "input_token_count": 1028, "output_token_count": 29, "latency": 193.93120908737183}
{"id": "multiple_64", "result": "[uv_index.get_future(location=\"\u6771\u4eac\", date=\"06-01-2023\")]", "input_token_count": 1065, "output_token_count": 25, "latency": 190.98309779167175}
{"id": "multiple_65", "result": "[geodistance.find(origin=\"New York\", destination=\"Los Angeles\")]", "input_token_count": 1095, "output_token_count": 18, "latency": 186.31388354301453}
{"id": "multiple_66", "result": "[traffic_estimate(start_location='Las Vegas', end_location='Los Angeles', time_period='weekend')]", "input_token_count": 1098, "output_token_count": 25, "latency": 186.81789588928223}
{"id": "multiple_67", "result": [{"translate": {"text": "\u4f60\u597d\uff0c\u4f60\u597d\u55ce\uff1f", "source_language": "en", "target_language": "fr"}}], "input_token_count": 955, "output_token_count": 37, "latency": 192.74975204467773}
{"id": "multiple_68", "result": "[library.search_books(location=\"New York\", genre=\"history\", title=\"\")]", "input_token_count": 1016, "output_token_count": 19, "latency": 185.09057664871216}
{"id": "multiple_69", "result": [{"five_factor_model.analyse": {"talkative": true, "nervous": true, "artistic_interests": false, "lazy": true, "forgiving": true}}], "input_token_count": 1295, "output_token_count": 49, "latency": 198.5447998046875}
{"id": "multiple_70", "result": "[european_history.get_monarchs(country='France', century=18)]", "input_token_count": 1306, "output_token_count": 20, "latency": 180.94351863861084}
{"id": "multiple_71", "result": "[get_population(year=1954, category=\"veterans\")]", "input_token_count": 1037, "output_token_count": 19, "latency": 179.70584297180176}
{"id": "multiple_72", "result": "[us_history.population_by_state_year(state=\"California\", year=1970)]", "input_token_count": 904, "output_token_count": 20, "latency": 180.80648255348206}
{"id": "multiple_73", "result": "[religion.get_origin(religion=\"\u4f5b\u6559\")]", "input_token_count": 695, "output_token_count": 15, "latency": 180.20122075080872}
{"id": "multiple_74", "result": "[art_auction.fetch_artwork_price(artwork_name=\"Starry Night\", artist=\"Van Gogh\")]", "input_token_count": 922, "output_token_count": 26, "latency": 184.9301073551178}
{"id": "multiple_75", "result": "[paint_color.trends(room=\"Living room\")]", "input_token_count": 1123, "output_token_count": 12, "latency": 168.91935515403748}
{"id": "multiple_76", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "multiple_77", "result": "[artwork_search.find(type=\"sculpture\", location=\"New York\", era=\"contemporary\")]", "input_token_count": 1140, "output_token_count": 24, "latency": 174.1631727218628}
{"id": "multiple_78", "result": [{"museum_info": {"museum": "\u502b\u6566\u81ea\u7136\u6b77\u53f2\u535a\u7269\u9928", "city": "\u502b\u6566", "features": ["timings", "exhibitions", "accessibility"]}}], "input_token_count": 1071, "output_token_count": 50, "latency": 187.32035303115845}
{"id": "multiple_79", "result": "[exhibition_info(museum_name=\"Museum of Modern Art\", month=1)]", "input_token_count": 803, "output_token_count": 19, "latency": 168.04628610610962}
{"id": "multiple_80", "result": "[music_shop.find_nearby(location=\"Nashville, TN\", services=[\"Violin Lessons\"], instruments=[\"Guitars\"])]", "input_token_count": 1175, "output_token_count": 31, "latency": 173.56197476387024}
{"id": "multiple_81", "result": "[concert.book_ticket(artist=\"Eminem\", location=\"New York\", add_ons=[\"Backstage Pass\"])]", "input_token_count": 1019, "output_token_count": 28, "latency": 169.30766439437866}
{"id": "multiple_82", "result": "[music.generate(key=\"C Major\", tempo=120)]", "input_token_count": 922, "output_token_count": 15, "latency": 158.05710196495056}
{"id": "multiple_83", "result": "[player_stats.get_all_time_goals(player_name=\"Lionel Messi\", team_name=\"FC Barcelona\")]", "input_token_count": 1134, "output_token_count": 25, "latency": 163.30040335655212}
{"id": "multiple_84", "result": "[getTopGoalScorers(competition=\"UEFA Champions League\", team=\"Barcelona\", number=10)]", "input_token_count": 916, "output_token_count": 25, "latency": 166.3835506439209}
{"id": "multiple_85", "result": "[basketball_scores.get_scores(team=\"\u7687\u5bb6\u99ac\u5fb7\u91cc\", league=\"\u897f\u7532\", rounds=5)]", "input_token_count": 933, "output_token_count": 24, "latency": 162.8439280986786}
{"id": "multiple_86", "result": "[BoardGameGeek.recommend(numPlayers=2, category=\"strategy\")]", "input_token_count": 1038, "output_token_count": 19, "latency": 159.95409536361694}
{"id": "multiple_87", "result": "[games.update.find(game=\"Cyberpunk 2077\", platform=\"Xbox\")]", "input_token_count": 1094, "output_token_count": 22, "latency": 153.3607521057129}
{"id": "multiple_88", "result": "[video_games.get_player_count(game_title=\"\u9b54\u7378\u4e16\u754c\", year=2020)]", "input_token_count": 937, "output_token_count": 23, "latency": 157.44192337989807}
{"id": "multiple_89", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "multiple_90", "result": "[restaurant.find_group(location=\"Seattle, WA\", cuisine=[\"Seafood\"], group_size=5)]", "input_token_count": 1000, "output_token_count": 22, "latency": 158.34802913665771}
{"id": "multiple_91", "result": "[recipe.find(mainIngredient=\"apple\", ingredientLimit=5)]", "input_token_count": 829, "output_token_count": 14, "latency": 150.55392813682556}
{"id": "multiple_92", "result": "[walmart.vegan_products(location=\"Denver, CO\", categories=[\"vegan\", \"gluten-free\"])]", "input_token_count": 1209, "output_token_count": 26, "latency": 155.35117197036743}
{"id": "multiple_93", "result": "[hotel.book(location=\"\u7d10\u7d04\", roomType=\"\u8c6a\u83ef\u5ba4\", nights=2, additional_services=[\"breakfast\"])]", "input_token_count": 1075, "output_token_count": 31, "latency": 158.99395418167114}
{"id": "multiple_94", "result": "[hotel_room_pricing.get(hotelName=\"Hilton New York\", roomType=\"suite\", nights=3)]", "input_token_count": 1240, "output_token_count": 27, "latency": 158.97822403907776}
{"id": "multiple_95", "result": [{"currency_exchange.convert": {"amount": 200, "from_currency": "EUR", "to_currency": "USD"}}], "input_token_count": 931, "output_token_count": 38, "latency": 165.73685336112976}
{"id": "multiple_96", "result": "[solve_quadratic_equation(a=2, b=6, c=5)]", "input_token_count": 1316, "output_token_count": 20, "latency": 154.28996968269348}
{"id": "multiple_97", "result": "[geometry.area_circle(radius=10)]", "input_token_count": 948, "output_token_count": 13, "latency": 144.26841259002686}
{"id": "multiple_98", "result": "[geometry.circumference(radius=3)]", "input_token_count": 1529, "output_token_count": 12, "latency": 145.41285729408264}
{"id": "multiple_99", "result": [{"calculus.derivative": {"function": "2*x**2", "value": 1}}], "input_token_count": 901, "output_token_count": 31, "latency": 154.98300123214722}
{"id": "multiple_100", "result": "[math.hcf(number1=36, number2=24)]", "input_token_count": 879, "output_token_count": 17, "latency": 147.98373079299927}
{"id": "multiple_101", "result": "[math.gcd(num1=12, num2=18)]", "input_token_count": 840, "output_token_count": 17, "latency": 151.43202352523804}
{"id": "multiple_102", "result": "[calculate_displacement(initial_velocity=10, time=5, acceleration=9.8)]", "input_token_count": 1227, "output_token_count": 22, "latency": 153.82243752479553}
{"id": "multiple_103", "result": "[calculate_final_speed(initial_velocity=0, height=100)]", "input_token_count": 1489, "output_token_count": 17, "latency": 149.76431345939636}
{"id": "multiple_104", "result": "[get_shortest_driving_distance(origin=\"\u7d10\u7d04\u5e02\", destination=\"\u83ef\u76db\u9813\u7279\u5340\")]", "input_token_count": 1415, "output_token_count": 26, "latency": 154.3544795513153}
{"id": "multiple_105", "result": "[calculate_magnetic_field(current=5, radius=4)]", "input_token_count": 1257, "output_token_count": 14, "latency": 146.2886757850647}
{"id": "multiple_106", "result": "[calculate_electric_field_strength(charge=0.01, distance=4)]", "input_token_count": 1612, "output_token_count": 20, "latency": 144.35253071784973}
{"id": "multiple_107", "result": "[calculate_density(mass=45, volume=15)]", "input_token_count": 917, "output_token_count": 16, "latency": 145.60650515556335}
{"id": "multiple_108", "result": [{"calc_heat_capacity": {"temp": 298, "volume": 10}}], "input_token_count": 974, "output_token_count": 31, "latency": 153.7686824798584}
{"id": "multiple_109", "result": "[cellbio.get_proteins(cell_compartment=\"plasma membrane\")]", "input_token_count": 1265, "output_token_count": 17, "latency": 148.3295817375183}
{"id": "multiple_110", "result": "[mutation_type.find(snp_id='rs6034464')]", "input_token_count": 946, "output_token_count": 19, "latency": 147.82590627670288}
{"id": "multiple_111", "result": [{"calculate_genotype_frequency": {"allele_frequency": 0.3, "genotype": "AA"}}], "input_token_count": 1108, "output_token_count": 34, "latency": 154.03284096717834}
{"id": "multiple_112", "result": "[forest_growth_forecast(location=\"Yellowstone National Park\", years=5, include_human_impact=True)]", "input_token_count": 823, "output_token_count": 26, "latency": 148.42159581184387}
{"id": "multiple_113", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "multiple_114", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "multiple_115", "result": "[find_restaurants(location=\"\u66fc\u54c8\u9813\", food_type=\"\u6cf0\u570b\", number=5, dietary_requirements=[\"\u7d20\u98df\"])]", "input_token_count": 1612, "output_token_count": 30, "latency": 152.42948198318481}
{"id": "multiple_116", "result": "[calculate_bmi(weight=85, height=180)]", "input_token_count": 865, "output_token_count": 17, "latency": 143.53612327575684}
{"id": "multiple_117", "result": "[calculate_BMI(weight_kg=70, height_m=1.75)]", "input_token_count": 1345, "output_token_count": 21, "latency": 144.0053162574768}
{"id": "multiple_118", "result": "[imdb.find_movies_by_actor(actor_name=\"Leonardo DiCaprio\", year=2010)]", "input_token_count": 1493, "output_token_count": 27, "latency": 146.5739405155182}
{"id": "multiple_119", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "multiple_120", "result": "[light_travel_time(distance_in_light_years=4)]", "input_token_count": 1756, "output_token_count": 15, "latency": 144.5658438205719}
{"id": "multiple_121", "result": "[geometry.area_triangle(base=6, height=10)]", "input_token_count": 1206, "output_token_count": 16, "latency": 144.6007752418518}
{"id": "multiple_122", "result": "[run_linear_regression(predictors=[\"age\", \"income\", \"education\"], target=\"publice_amount\", standardize=True)]", "input_token_count": 1635, "output_token_count": 30, "latency": 150.96007299423218}
{"id": "multiple_123", "result": [{"calculate_probability": {"total_outcomes": 52, "favorable_outcomes": 4, "round_to": 2}}], "input_token_count": 1230, "output_token_count": 39, "latency": 156.80041766166687}
{"id": "multiple_124", "result": "[probabilities.calculate_single(total_outcomes=52, event_outcomes=1)]", "input_token_count": 1225, "output_token_count": 19, "latency": 145.24096012115479}
{"id": "multiple_125", "result": "[run_two_sample_ttest(group1=[3, 4, 5, 6, 4], group2=[7, 8, 9, 8, 7], equal_variance=True)]", "input_token_count": 1309, "output_token_count": 48, "latency": 160.23395586013794}
{"id": "multiple_126", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "multiple_127", "result": "[finance.calculate_quarterly_dividend_per_share(total_payout=5000000, outstanding_shares=10000000)]", "input_token_count": 1533, "output_token_count": 39, "latency": 155.60605025291443}
{"id": "multiple_128", "result": "[calculate_return_on_equity(net_income=2000000, shareholder_equity=10000000, dividends_paid=200000)]", "input_token_count": 1754, "output_token_count": 48, "latency": 163.21284222602844}
{"id": "multiple_129", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "multiple_130", "result": "[calculate_cagr(initial_value=2000, final_value=3000, period_in_years=4)]", "input_token_count": 1196, "output_token_count": 28, "latency": 151.90891289710999}
{"id": "multiple_131", "result": "[market_performance.get_data(indexes=[\"S&P 500\", \"Dow Jones\"], days=5)]", "input_token_count": 1378, "output_token_count": 25, "latency": 149.52462887763977}
{"id": "multiple_132", "result": "[finance.calculate_future_value(initial_investment=20000, rate_of_return=0.08, years=5)]", "input_token_count": 1291, "output_token_count": 32, "latency": 153.14853310585022}
{"id": "multiple_133", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "multiple_134", "result": "[crime_record.get_record(case_number=\"CA123456\", county=\"San Diego\", details=True)]", "input_token_count": 1711, "output_token_count": 27, "latency": 151.96273279190063}
{"id": "multiple_135", "result": "[get_case_info(docket=\"2022/AL2562\", court=\"\u52a0\u5229\u798f\u5c3c\u4e9e\u5dde\", info_type=\"victim\")]", "input_token_count": 905, "output_token_count": 36, "latency": 155.49058151245117}
{"id": "multiple_136", "result": "[get_crime_rate(city=\"San Francisco\", state=\"California\", year=2020)]", "input_token_count": 962, "output_token_count": 23, "latency": 144.3770546913147}
{"id": "multiple_137", "result": "[lawsuit_search(company=\"Google\", start_date=\"2021-01-01\", location=\"California\", status=\"ongoing\")]", "input_token_count": 1378, "output_token_count": 35, "latency": 151.35988593101501}
{"id": "multiple_138", "result": "[legal_case.fetch(case_id=\"R VS Adams\", details=True)]", "input_token_count": 1250, "output_token_count": 16, "latency": 135.46774554252625}
{"id": "multiple_139", "result": "[lawsuit_details.find(company_name=\"\u860b\u679c\u516c\u53f8\", year=2010)]", "input_token_count": 1673, "output_token_count": 23, "latency": 138.38399696350098}
{"id": "multiple_140", "result": "[lawsuits_search(company_name=\"Google\", location=\"\u52a0\u5229\u798f\u5c3c\u4e9e\u5dde\", year=2020)]", "input_token_count": 1161, "output_token_count": 27, "latency": 134.31224966049194}
{"id": "multiple_141", "result": "[lawsuit.check_case(case_id=1234, closed_status=True)]", "input_token_count": 1145, "output_token_count": 20, "latency": 128.85799717903137}
{"id": "multiple_142", "result": "[weather.humidity_forecast(location=\"\u4f5b\u7f85\u91cc\u9054\u5dde\u9081\u963f\u5bc6\", days=7)]", "input_token_count": 952, "output_token_count": 24, "latency": 131.26500248908997}
{"id": "multiple_143", "result": "[calculate_slope_gradient(point1=[40.7128, -74.0060], point2=[34.0522, -118.2437])]", "input_token_count": 1541, "output_token_count": 49, "latency": 146.64036893844604}
{"id": "multiple_144", "result": "[air_quality(location=\"London\", date=\"2022-08-16\")]", "input_token_count": 836, "output_token_count": 23, "latency": 128.3330135345459}
{"id": "multiple_145", "result": "[calculate_emissions(distance=12000, fuel_type='gasoline', fuel_efficiency=20)]", "input_token_count": 1345, "output_token_count": 27, "latency": 130.66659307479858}
{"id": "multiple_146", "result": "[restaurant.find_nearby(location=\"Seattle, WA\", cuisine=\"Chinese\", max_distance=10)]", "input_token_count": 902, "output_token_count": 24, "latency": 128.2457046508789}
{"id": "multiple_147", "result": "[map_service.get_directions(start=\"New York\", end=\"Los Angeles\", avoid=[\"highways\", \"tolls\"])]", "input_token_count": 1261, "output_token_count": 28, "latency": 130.12963581085205}
{"id": "multiple_148", "result": "[get_stock_info(company_name='Apple', detail_level='detailed')]", "input_token_count": 875, "output_token_count": 18, "latency": 122.25366187095642}
{"id": "multiple_149", "result": [{"sentiment_analysis": {"text": "\u6211\u559c\u6b61\u9019\u88e1\u7684\u98df\u7269\uff01\u5b83\u7e3d\u662f\u65b0\u9bae\u53c8\u7f8e\u5473\u7684\u3002", "language": "zh"}}], "input_token_count": 1563, "output_token_count": 48, "latency": 139.95389032363892}
{"id": "multiple_150", "result": "[calculate_neuronal_activity(input_synaptic_rate=200, weight=0.5, decay_rate=0.1)]", "input_token_count": 1793, "output_token_count": 29, "latency": 129.37823915481567}
{"id": "multiple_151", "result": [{"social_media_analytics.most_followed": {"topic": "\u5fc3\u7406\u5b78", "sub_topics": ["\u884c\u70ba", "\u7fa4\u9ad4\u52d5\u614b"]}}], "input_token_count": 1287, "output_token_count": 44, "latency": 140.10302996635437}
{"id": "multiple_152", "result": "[history.get_key_events(country=\"Germany\", start_year=1871, end_year=1945, event_type=[\"War\"])]", "input_token_count": 1086, "output_token_count": 33, "latency": 139.4894106388092}
{"id": "multiple_153", "result": "[get_event_date(event=\"Lisbon Treaty\")]", "input_token_count": 1193, "output_token_count": 13, "latency": 115.1371750831604}
{"id": "multiple_154", "result": "[US_president.in_year(year=1861, full_name=True)]", "input_token_count": 1614, "output_token_count": 18, "latency": 117.5599524974823}
{"id": "multiple_155", "result": "[get_discoverer(discovery=\"\u4e2d\u5b50\", detail=True)]", "input_token_count": 1209, "output_token_count": 16, "latency": 114.49175071716309}
{"id": "multiple_156", "result": "[historical_contrib.get_contrib(scientist=\"Albert Einstein\", date=\"1915-03-17\")]", "input_token_count": 1212, "output_token_count": 30, "latency": 130.23037552833557}
{"id": "multiple_157", "result": "[get_earliest_reference(name=\"\u8036\u7a4c\u57fa\u7763\", source=\"historical records\")]", "input_token_count": 866, "output_token_count": 22, "latency": 124.37847232818604}
{"id": "multiple_158", "result": "[religious_history.get_papal_biography(papal_name=\"Innocent III\", include_contributions=True)]", "input_token_count": 1637, "output_token_count": 27, "latency": 126.14243984222412}
{"id": "multiple_159", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "multiple_160", "result": "[get_sculpture_info(artist_name=\"James Plensa\", detail=True)]", "input_token_count": 1570, "output_token_count": 19, "latency": 118.30214023590088}
{"id": "multiple_161", "result": "[find_exhibition(location=\"New York, NY\", art_form=\"sculpture\", month=\"next\", user_ratings=\"high\")]", "input_token_count": 1372, "output_token_count": 31, "latency": 124.74094653129578}
{"id": "multiple_162", "result": "[analyze_structure(building_id='B1004', floors=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 83", "input_token_count": 920, "output_token_count": 4096, "latency": 539.6407380104065}
{"id": "multiple_163", "result": "[metropolitan_museum.get_top_artworks(number=5, sort_by='popularity')]", "input_token_count": 1000, "output_token_count": 22, "latency": 118.36704134941101}
{"id": "multiple_164", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "multiple_165", "result": "[guitar_price.find(model=\"Gibson Les Paul\", condition=\"Good\", location=\"Chicago\")]", "input_token_count": 1196, "output_token_count": 22, "latency": 112.2250428199768}
{"id": "multiple_166", "result": "[concert.search(genre=\"classical\", location=\"Los Angeles\", date=\"this weekend\", price_range=\"cheap\")]", "input_token_count": 1302, "output_token_count": 28, "latency": 115.68234181404114}
{"id": "multiple_167", "result": "[music_generator.generate_melody(key='C', start_note='C4', length=16, tempo=120)]", "input_token_count": 1400, "output_token_count": 29, "latency": 119.34531497955322}
{"id": "multiple_168", "result": "[get_song_lyrics(song_title=\"\u6ce2\u5e0c\u7c73\u4e9e\u72c2\u60f3\u66f2\", artist_name=\"\u7687\u540e\")]", "input_token_count": 1158, "output_token_count": 28, "latency": 117.53777003288269}
{"id": "multiple_169", "result": "[musical_scale(key=\"C#\", scale_type=\"major\")]", "input_token_count": 889, "output_token_count": 16, "latency": 111.00949192047119}
{"id": "multiple_170", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "multiple_171", "result": "[game_result.get_winner(teams=[\"\u6e56\u4eba\", \"\u5feb\u8239\"], date=\"2021-01-28\")]", "input_token_count": 891, "output_token_count": 31, "latency": 118.7278938293457}
{"id": "multiple_172", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "multiple_173", "result": "[get_defense_ranking(season=2021)]", "input_token_count": 1156, "output_token_count": 15, "latency": 108.6767041683197}
{"id": "multiple_174", "result": "[sports_ranking(team=\"\u66fc\u806f\", league=\"\u82f1\u8d85\")]", "input_token_count": 977, "output_token_count": 17, "latency": 111.67904448509216}
{"id": "multiple_175", "result": "[sports_ranking.get_top_player(sport='tennis', gender='women')]", "input_token_count": 1614, "output_token_count": 20, "latency": 113.64645338058472}
{"id": "multiple_176", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "multiple_177", "result": "[board_game.chess.get_top_players(location=\"New York\", minimum_rating=2300)]", "input_token_count": 1256, "output_token_count": 23, "latency": 115.82963037490845}
{"id": "multiple_178", "result": "[find_card_in_deck(rank=\"Queen\", suit=\"Queen\")]", "input_token_count": 1780, "output_token_count": 17, "latency": 111.78803133964539}
{"id": "multiple_179", "result": "[poker_probability.full_house(deck_size=52, hand_size=5)]", "input_token_count": 1281, "output_token_count": 20, "latency": 112.87813782691956}
{"id": "multiple_180", "result": "[game_stats.fetch_player_statistics(game=\"Zelda\", username=\"SAM\")]", "input_token_count": 1583, "output_token_count": 20, "latency": 113.50810647010803}
{"id": "multiple_181", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "multiple_182", "result": "[multiplayer_game_finder(platform='Windows 10', rating=4.5)]", "input_token_count": 1767, "output_token_count": 21, "latency": 113.9205572605133}
{"id": "multiple_183", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "multiple_184", "result": [{"recipe_search": {"dietary_restriction": "Vegetarian", "ingredients": ["tofu", "pasta", "tomatoes", "cheese"], "servings": 2}}], "input_token_count": 1191, "output_token_count": 52, "latency": 130.4221169948578}
{"id": "multiple_185", "result": [{"restaurant_search.find_closest": {"location": "Boston, MA", "cuisine": "Sushi"}}], "input_token_count": 985, "output_token_count": 33, "latency": 118.50650691986084}
{"id": "multiple_186", "result": "[find_recipe(dietary_restrictions=\"vegan\", recipe_type=\"dessert\", time=30)]", "input_token_count": 917, "output_token_count": 25, "latency": 113.14667654037476}
{"id": "multiple_187", "result": "[whole_foods.check_price(location=\"Los Angeles\", items=[\"tomatoes\", \"lettuce\"])]", "input_token_count": 1515, "output_token_count": 23, "latency": 110.04401874542236}
{"id": "multiple_188", "result": "[grocery_store.find_best(my_location=\"Berkeley\", rating=4.5, products=[\"tomatoes\", \"pet food\"])]", "input_token_count": 1579, "output_token_count": 30, "latency": 114.29497027397156}
{"id": "multiple_189", "result": "[timezone.convert(time=\"3pm\", from_timezone=\"America/New_York\", to_timezone=\"Europe/London\")]", "input_token_count": 893, "output_token_count": 28, "latency": 112.03334522247314}
{"id": "multiple_190", "result": "[book_hotel(hotel_name=\"\u5e0c\u723e\u9813\", location=\"\u829d\u52a0\u54e5\", room_type=\"\u55ae\u9593\", start_date=\"2022-12-10\", nights=2)]", "input_token_count": 1251, "output_token_count": 44, "latency": 120.64958024024963}
{"id": "multiple_191", "result": "[book_hotel(hotel_name=\"Las Vegas Hotel\", location=\"Las Vegas\", room_type=\"Luxury\", start_date=\"05-12-2022\", stay_duration=3, view=\"City View\")]", "input_token_count": 1673, "output_token_count": 50, "latency": 126.209876537323}
{"id": "multiple_192", "result": [{"currency_conversion.convert": {"amount": 150, "from_currency": "EUR", "to_currency": "CAD"}}], "input_token_count": 1189, "output_token_count": 38, "latency": 119.03396892547607}
{"id": "multiple_193", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "multiple_194", "result": "[get_museum_hours(museum_name=\"\u9031\u516d\u5927\u90fd\u6703\u85dd\u8853\u535a\u7269\u9928\", day=\"Saturday\")]", "input_token_count": 1551, "output_token_count": 28, "latency": 113.41812682151794}
{"id": "multiple_195", "result": "Error during inference: 'arguments'", "traceback": "Traceback (most recent call last):\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/base_oss_handler.py\", line 279, in _multi_threaded_inference\n    model_responses, metadata = self.inference_single_turn_prompting(\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/base_handler.py\", line 631, in inference_single_turn_prompting\n    model_response_data = self._parse_query_response_prompting(api_response)\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 276, in _parse_query_response_prompting\n    model_responses = [\n  File \"/workspace/gorilla/berkeley-function-call-leaderboard/bfcl_eval/model_handler/local_inference/mistral_fc.py\", line 277, in <listcomp>\n    {item[\"name\"]: item[\"arguments\"]} for item in model_responses\nKeyError: 'arguments'\n"}
{"id": "multiple_196", "result": "[cellbio.get_proteins(cell_compartment=\"plasma membrane\")]", "input_token_count": 1513, "output_token_count": 17, "latency": 105.14776659011841}
{"id": "multiple_197", "result": "[mutation_type.find(snp_id=\"rs6034464\")]", "input_token_count": 1646, "output_token_count": 19, "latency": 105.9471504688263}
{"id": "multiple_198", "result": [{"calculate_genotype_frequency": {"allele_frequency": 0.3, "genotype": "AA"}}], "input_token_count": 1670, "output_token_count": 34, "latency": 117.75665497779846}
{"id": "multiple_199", "result": [{"forest_growth_forecast": {"location": "\u9ec3\u77f3", "years": 5, "include_human_impact": true}}], "input_token_count": 1108, "output_token_count": 39, "latency": 120.7472882270813}
